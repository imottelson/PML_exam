{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613013de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyro\n",
    "import torch\n",
    "import pyro.contrib.gp as gp\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import MCMC, NUTS\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = '1'\n",
    "torch.set_num_threads(4)\n",
    "pyro.set_rng_seed(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b171b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return torch.sin(20*x) +2*torch.cos(14*x) -2*torch.sin(6*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3aea8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the gaussian process\n",
    "X = torch.tensor([-1,-.5,0,.5,1])\n",
    "y = f(X)\n",
    "kernel = gp.kernels.RBF(input_dim=1)\n",
    "gpr = gp.models.GPRegression(X, y, kernel,noise=torch.tensor(10**-4))\n",
    "gpr.kernel.variance = pyro.nn.PyroSample(dist.LogNormal(torch.tensor(0.), torch.tensor(2.0)))\n",
    "gpr.kernel.lengthscale = pyro.nn.PyroSample(dist.LogNormal(torch.tensor(-1.0), torch.tensor(1.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d821327e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run MCMC sampler\n",
    "num_chains=4\n",
    "hmc_kernel = NUTS(gpr.model)\n",
    "mcmc = MCMC(hmc_kernel, num_samples=500, num_chains=num_chains,\n",
    "            mp_context='spawn', warmup_steps=10)\n",
    "mcmc.run()\n",
    "samples = mcmc.get_samples(group_by_chain=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e585f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect MCMC quality control metrics\n",
    "print(az.summary(samples))\n",
    "ax1= az.plot_trace(samples,transform=np.log,figsize=(12,8))\n",
    "plt.savefig('MCMC_QC1.png')\n",
    "az.plot_autocorr(samples,combined=True,max_lag=20,)\n",
    "plt.savefig('MCMC_QC2.png')\n",
    "az.plot_ess(samples,kind='local')\n",
    "plt.savefig('MCMC_QC3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b91a684",
   "metadata": {},
   "outputs": [],
   "source": [
    "#B.1.3 calculate mean and variance at each point by integrating over posterior samples\n",
    "Xtest= torch.linspace(-1.0,1., 200)\n",
    "subsample = mcmc.get_samples(500)\n",
    "mean_list = []\n",
    "var_list = []\n",
    "for post_samp in range(0, 500):\n",
    "    pyro.clear_param_store()\n",
    "    kernel = gp.kernels.RBF(input_dim=1)\n",
    "    kernel.variance = subsample['kernel.variance'][post_samp]\n",
    "    kernel.lengthscale = subsample['kernel.lengthscale'][post_samp]\n",
    "    gpr_post = gp.models.GPRegression(X, y, kernel,noise=torch.tensor(0.0001))\n",
    "    post_pred = gpr_post(Xtest,full_cov=False, noiseless=False)\n",
    "    mean_list.append(post_pred[0])\n",
    "    var_list.append(post_pred[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d7e24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = sum(mean_list)/len(mean_list)\n",
    "var = (sum(var_list) + sum([i**2 for i in mean_list]))/len(var_list) - mean**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36566208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B.1.(1/3)\n",
    "fig, (ax1,ax2) = plt.subplots(figsize=(10, 4), ncols=2)\n",
    "fig.tight_layout(pad=4)\n",
    "ax1.set_title('A', loc='left')\n",
    "priorlen = dist.LogNormal(-1,1).sample(sample_shape=(500,))\n",
    "priorvar = dist.LogNormal(0,2).sample(sample_shape=(500,))\n",
    "ax1.scatter(priorvar, priorlen, s=7, label='prior')\n",
    "ax1.scatter(subsample['kernel.variance'],\n",
    "            subsample['kernel.lengthscale'], s=7, label='posterior')\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_yscale('log')\n",
    "ax1.set_xlabel('kernel variance')\n",
    "ax1.set_ylabel('kernel lengthscale')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.set_title('B', loc='left')\n",
    "with torch.no_grad():\n",
    "    ax2.plot(X.numpy(), y.numpy(), \"kx\", label='$\\mathcal{D}$')\n",
    "    sd = var.sqrt()  # standard deviation at each input point x\n",
    "    ax2.plot(Xtest.numpy(), mean.numpy(), \"r\", lw=2, label='m(x)')  # plot the mean\n",
    "    ax2.fill_between(\n",
    "            Xtest.numpy(),  # plot the two-sigma uncertainty about the mean\n",
    "            (mean - 2.0 * sd).numpy(),\n",
    "            (mean + 2.0 * sd).numpy(),\n",
    "            color=\"C0\",\n",
    "            alpha=0.3,\n",
    "            label='2$\\sigma$ CI'\n",
    "        )\n",
    "ax2.plot(Xtest, f(Xtest), \"b\", lw=2, label='f(x)')\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('y')\n",
    "ax2.legend(loc= 'lower right')\n",
    "\n",
    "plt.savefig('GP.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1d594d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for conducting bayesian optimization and plotting it\n",
    "def bayesian_opt(X,y,loss_method, kappa=2):\n",
    "    global seed\n",
    "    # estimate  theta|X,y\n",
    "    print(i)\n",
    "    for attempt in range(0,10):\n",
    "        pyro.clear_param_store()\n",
    "        kernel = gp.kernels.RBF(input_dim=1)\n",
    "        kernel.variance = pyro.nn.PyroSample(dist.LogNormal(torch.tensor(0.), torch.tensor(2.0)))\n",
    "        kernel.lengthscale = pyro.nn.PyroSample(dist.LogNormal(torch.tensor(-1.0), torch.tensor(1.0)))\n",
    "        gpr_opt = gp.models.GPRegression(X, y, kernel,noise=torch.tensor(10**-4))\n",
    "        hmc_kernel = NUTS(gpr_opt.model)#, jit_compile=True)\n",
    "        mcmc = MCMC(hmc_kernel, num_samples=100, num_chains=1,\n",
    "                mp_context='spawn', warmup_steps=10, disable_progbar=True)\n",
    "        mcmc.run()\n",
    "        diagn = mcmc.diagnostics()\n",
    "        if (diagn['kernel.variance']['r_hat']<=1.05) & (diagn['kernel.variance']['r_hat']<=1.05):\n",
    "            break\n",
    "        seed+=1\n",
    "        pyro.set_rng_seed(seed)\n",
    "    else:\n",
    "        raise Exception('No convergence after 10 attempts.')\n",
    "\n",
    "    posterior = mcmc.get_samples(1)\n",
    "    # calculate p(f*|X*,theta)\n",
    "    pyro.clear_param_store()\n",
    "    kernel = gp.kernels.RBF(input_dim=1)\n",
    "    kernel.variance = posterior['kernel.variance']\n",
    "    kernel.lengthscale = posterior['kernel.lengthscale']\n",
    "    gpr_opt = gp.models.GPRegression(X, y, kernel, noise=torch.tensor(10**-4))\n",
    "    mean, cov = gpr_opt(Xtest, full_cov=True, noiseless=False)\n",
    "    sd = cov.diag().sqrt()\n",
    "    cov += 1e-3 * torch.eye(len(Xtest)) # provides numerical stability\n",
    "\n",
    "    # sample f*\n",
    "    fstar = dist.MultivariateNormal(\n",
    "        mean, covariance_matrix=cov, \n",
    "    ).sample()\n",
    "    # find argmin f*\n",
    "    if loss_method=='fmin':\n",
    "        Xstar = Xtest[torch.argmin(fstar)]\n",
    "    elif loss_method=='LCM':\n",
    "        LCB = mean - kappa*sd\n",
    "        Xstar = Xtest[torch.argmin(LCB)]\n",
    "    else:\n",
    "        raise Exception('Loss method must be one of \"fmin\" or \"LCM\".')\n",
    "    X = torch.cat((X, Xstar.reshape(1)))\n",
    "    y = f(X)\n",
    "    seed+=1\n",
    "    pyro.set_rng_seed(seed)\n",
    "    return(X,y,mean,cov,Xstar,fstar)\n",
    "\n",
    "def bay_opt_plot(X,y,mean,cov,Xstar,fstar, timestep, ax, legend=None):\n",
    "    ax.plot(Xtest.numpy(), fstar.numpy().T, lw=2, alpha=0.4, label='f*(x)')\n",
    "    ax.plot(Xtest.numpy(), f(Xtest), lw=2, alpha=0.4, label='f(x)')\n",
    "    ax.plot(X.numpy(), y.numpy(), \"kx\", label='$\\mathcal{D}$')\n",
    "    sd = cov.diag().sqrt()  # standard deviation at each input point x\n",
    "    ax.plot(Xtest.numpy(), mean.numpy(), \"r\", lw=2, label='m(x)')  # plot the mean\n",
    "    ax.fill_between(\n",
    "            Xtest.numpy(),  # plot the two-sigma uncertainty about the mean\n",
    "            (mean - 2.0 * sd).numpy(),\n",
    "            (mean + 2.0 * sd).numpy(),\n",
    "            color=\"C0\",\n",
    "            alpha=0.3,\n",
    "            label='2$\\sigma$ CI'\n",
    "    )\n",
    "    ax.plot(Xstar.numpy(), f(Xstar).numpy(), \"bo\", markersize=10, label='$(x^*_p, f(x^*_p)$')\n",
    "    if legend != None:\n",
    "        ax.legend(loc=legend,prop={'size': 6})\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title(f\"Iteration: {timestep}\", loc='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9424fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make progression plot for B.2.1\n",
    "seed=1\n",
    "Xtest= torch.linspace(-1., 1, 200)\n",
    "loss_method = 'fmin'\n",
    "fig, ax = plt.subplots(figsize=(8, 3),ncols=3)\n",
    "X = torch.tensor([-1,-.5,0,.5,1])\n",
    "y = f(X)\n",
    "for i in range(0,11):\n",
    "    X,y,mean,cov,Xstar,fstar = bayesian_opt(X,y,loss_method, kappa=2)    \n",
    "    pyro.set_rng_seed(seed)\n",
    "    # plot\n",
    "    if i%5 == 0:\n",
    "        n = i//5\n",
    "        if n==0:\n",
    "            legend= 'lower left'\n",
    "        else:\n",
    "            legend= None\n",
    "        with torch.no_grad():\n",
    "            bay_opt_plot(X,y,mean,cov,Xstar,fstar, timestep=i, ax=ax[n], legend=legend)\n",
    "plt.savefig(f'bayopt.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee63944",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1\n",
    "X = torch.tensor([-1,-.5,0,.5,1])\n",
    "y = f(X)\n",
    "Xtest= torch.linspace(-1., 1, 200)\n",
    "loss_method = 'fmin'\n",
    "for j in range(0,10):\n",
    "    loss_method = 'fmin'\n",
    "    fig, ax = plt.subplots(figsize=(6,4),nrows=1)\n",
    "    X = torch.tensor([-1,-.5,0,.5,1])\n",
    "    y = f(X)\n",
    "    for i in range(0,11):\n",
    "        X,y,mean,cov,Xstar,fstar = bayesian_opt(X,y,loss_method, kappa=2)    \n",
    "        pyro.set_rng_seed(seed)\n",
    "        # plot\n",
    "        if i== 10:\n",
    "            with torch.no_grad():\n",
    "                bay_opt_plot(X,y,mean,cov,Xstar,fstar, timestep=i, ax=ax,legend= 'lower left')\n",
    "    plt.savefig(f'PML_finals/bayopt{j}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1639c780",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1\n",
    "X = torch.tensor([-1,-.5,0,.5,1])\n",
    "y = f(X)\n",
    "Xtest= torch.linspace(-1., 1, 200)\n",
    "loss_method = 'LCM'\n",
    "for j in range(0,10):\n",
    "    fig, ax = plt.subplots(figsize=(6, 4),nrows=1)\n",
    "    X = torch.tensor([-1,-.5,0,.5,1])\n",
    "    y = f(X)\n",
    "    for i in range(0,11):\n",
    "        X,y,mean,cov,Xstar,fstar = bayesian_opt(X,y,loss_method, kappa=2)    \n",
    "        pyro.set_rng_seed(seed)\n",
    "        # plot\n",
    "        if i == 10:\n",
    "            n = i//5\n",
    "            with torch.no_grad():\n",
    "                bay_opt_plot(X,y,mean,cov,Xstar,fstar, timestep=i, ax=ax,legend= 'lower left')\n",
    "    plt.savefig(f'PML_finals/bayopt_LCM{j}.png')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-PML]",
   "language": "python",
   "name": "conda-env-.conda-PML-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
